# -*- coding: utf-8 -*-
"""Music2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m2XmUHAil0_SqVVZhiatzfonhtenWWSp
"""

!sudo apt install -y fluidsynth

!pip install --upgrade pyfluidsynth

!pip install pretty_midi

import collections
import datetime
import fluidsynth
import glob
import numpy as np
import pathlib
import pandas as pd
import pretty_midi
import seaborn as sns
import tensorflow as tf

from IPython import display
from matplotlib import pyplot as plt
from typing import Dict,List, Optional, Sequence, Tuple

# Setting the random seed for reproducibility
# This ensures that the same random values are generated each time the code is run
initial_seed_value = 42

# Setting TensorFlow's random seed to make results deterministic
tf.random.set_seed(initial_seed_value)

# Setting the NumPy random seed to control the randomness in NumPy operations
np.random.seed(initial_seed_value)

# Defining the sampling rate used for audio processing or playback (in Hertz)
AUDIO_SAMPLE_RATE = 16000

# Define the directory path where the dataset is stored
dataset_directory = pathlib.Path('data/maestro-v2.0.0')

# Check if the dataset directory exists, if not, download the dataset
if not dataset_directory.exists():
    # Download the MAESTRO v2.0.0 MIDI dataset and extract it
    tf.keras.utils.get_file(
        'maestro-v2.0.0-midi.zip',  # Name of the downloaded file
        origin='https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip',  # Dataset URL
        extract=True,  # Extract the contents once downloaded
        cache_dir='.',  # Specify the directory to store the cache
        cache_subdir='data',  # Subdirectory within the cache to place data
    )

# Get a list of all MIDI files in the dataset directory (recursively searching through subdirectories)
midi_file_paths = glob.glob(str(dataset_directory / '**/*.mid*'))

# Output the total number of MIDI files found
print('Total number of MIDI files:', len(midi_file_paths))

# Select a sample MIDI file from the list of found files (taking the second file in the list)
selected_sample_file = midi_file_paths[1]

# Print the path of the selected sample file
print('Sample MIDI file:', selected_sample_file)

# Load the selected sample MIDI file using PrettyMIDI for further analysis or processing
midi_data = pretty_midi.PrettyMIDI(selected_sample_file)

# Function to play a segment of the audio generated from the MIDI file
# 'midi_object' is the PrettyMIDI object, and 'duration' specifies how many seconds of audio to generate
def play_midi_audio(midi_object: pretty_midi.PrettyMIDI, duration=30):
    # Generate the waveform from the MIDI file using FluidSynth
    audio_waveform = midi_object.fluidsynth(fs=AUDIO_SAMPLE_RATE)

    # Trim the generated waveform to the specified duration to avoid overloading the system
    trimmed_waveform = audio_waveform[:duration * AUDIO_SAMPLE_RATE]

    # Return an audio player widget for playback within the display
    return display.Audio(trimmed_waveform, rate=AUDIO_SAMPLE_RATE)

# Call the function to play the audio generated from the MIDI file using the PrettyMIDI object
play_midi_audio(midi_data)

# Print the total number of instruments detected in the MIDI file
print('Number of instruments:', len(midi_data.instruments))

# Select the first instrument from the list of instruments
first_instrument = midi_data.instruments[0]

# Get the name of the selected instrument using its program number
instrument_name = pretty_midi.program_to_instrument_name(first_instrument.program)

# Print the name of the selected instrument
print('Name of the first instrument:', instrument_name)

# Print information about the first 10 notes of the selected instrument
for index, midi_note in enumerate(first_instrument.notes[:10]):
    # Convert the MIDI note number to its corresponding note name
    note_name = pretty_midi.note_number_to_name(midi_note.pitch)

    # Calculate the duration of the note
    note_duration = midi_note.end - midi_note.start

    # Print details of each note, including its index, pitch, name, and duration
    print(f'{index}: pitch={midi_note.pitch}, note_name={note_name},'
          f' duration={note_duration:.4f}')

def midi_to_notes(midi_file: str) -> pd.DataFrame:
  pm = pretty_midi.PrettyMIDI(midi_file)
  instrument = pm.instruments[0]
  notes = collections.defaultdict(list)

  # Sort the notes by start time
  sorted_notes = sorted(instrument.notes, key=lambda note: note.start)
  prev_start = sorted_notes[0].start

  for note in sorted_notes:
    start = note.start
    end = note.end
    notes['pitch'].append(note.pitch)
    notes['start'].append(start)
    notes['end'].append(end)
    notes['step'].append(start - prev_start)
    notes['duration'].append(end - start)
    prev_start = start

  return pd.DataFrame({name: np.array(value) for name, value in notes.items()})

# Convert MIDI file to a DataFrame of notes using the 'midi_to_notes' function
notes_dataframe = midi_to_notes(selected_sample_file)

# Display the first few rows of the notes DataFrame
notes_dataframe.head()

# Vectorize the function to convert MIDI note numbers to note names for efficient processing
convert_to_note_names = np.vectorize(pretty_midi.note_number_to_name)

# Apply the vectorized function to the 'pitch' column of the notes DataFrame to get note names
note_names_array = convert_to_note_names(notes_dataframe['pitch'])

# Display the first 10 note names
note_names_array[:10]

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from typing import Optional

# Function to plot a piano roll visualization of MIDI notes
# 'notes' is a DataFrame containing note information, and 'count' specifies the number of notes to display
def plot_piano_roll(notes_dataframe: pd.DataFrame, num_notes: Optional[int] = None):
    # Set the plot title based on the number of notes to display
    if num_notes:
        plot_title = f'First {num_notes} notes'
    else:
        plot_title = 'Whole track'
        num_notes = len(notes_dataframe['pitch'])

    # Create a figure for plotting
    plt.figure(figsize=(20, 4))

    # Prepare data for plotting
    pitch_values = np.stack([notes_dataframe['pitch'], notes_dataframe['pitch']], axis=0)
    time_values = np.stack([notes_dataframe['start'], notes_dataframe['end']], axis=0)

    # Plot the piano roll
    plt.plot(
        time_values[:, :num_notes], pitch_values[:, :num_notes], color="b", marker="."
    )

    # Set plot labels and title
    plt.xlabel('Time [s]')
    plt.ylabel('Pitch')
    plt.title(plot_title)

    # Display the plot
    plt.show()

# Call the function to plot the piano roll for the first 100 notes
plot_piano_roll(notes_dataframe, num_notes=100)

# Call the function to plot the piano roll for the entire track
plot_piano_roll(notes_dataframe)

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Function to plot distributions of MIDI note characteristics
# 'notes_dataframe' is a DataFrame containing note information, and 'drop_percentile' determines the percentile threshold for the plots
def plot_distributions(notes_dataframe: pd.DataFrame, drop_percentile=2.5):
    plt.figure(figsize=[15, 5])

    # Plot the distribution of note pitches
    plt.subplot(1, 3, 1)
    sns.histplot(notes_dataframe, x="pitch", bins=20)
    plt.xlabel('Pitch')
    plt.title('Pitch Distribution')

    # Plot the distribution of note steps, excluding outliers based on drop_percentile
    plt.subplot(1, 3, 2)
    max_step_value = np.percentile(notes_dataframe['step'], 100 - drop_percentile)
    sns.histplot(notes_dataframe, x="step", bins=np.linspace(0, max_step_value, 21))
    plt.xlabel('Step')
    plt.title('Step Distribution')

    # Plot the distribution of note durations, excluding outliers based on drop_percentile
    plt.subplot(1, 3, 3)
    max_duration_value = np.percentile(notes_dataframe['duration'], 100 - drop_percentile)
    sns.histplot(notes_dataframe, x="duration", bins=np.linspace(0, max_duration_value, 21))
    plt.xlabel('Duration')
    plt.title('Duration Distribution')

    plt.tight_layout()
    plt.show()

# Call the function to plot the distributions of MIDI note characteristics
plot_distributions(notes_dataframe)

import pandas as pd  # Import pandas for data manipulation
import pretty_midi  # Import pretty_midi for MIDI file creation and manipulation

# Function to convert a DataFrame of notes to a MIDI file
# 'notes_dataframe' contains note data, 'out_file' is the path for saving the MIDI file,
# 'instrument_name' specifies the instrument, and 'velocity' controls note loudness
def notes_to_midi(
    notes_dataframe: pd.DataFrame,
    output_file: str,
    instrument_name: str,
    velocity: int = 100  # Note loudness
) -> pretty_midi.PrettyMIDI:

    # Create a new PrettyMIDI object
    midi_file = pretty_midi.PrettyMIDI()

    # Create an instrument with the specified name
    instrument = pretty_midi.Instrument(
        program=pretty_midi.instrument_name_to_program(instrument_name)
    )

    # Initialize the start time of the previous note
    previous_start_time = 0

    # Iterate over each note in the DataFrame
    for _, note_row in notes_dataframe.iterrows():
        # Calculate start and end times for the current note
        start_time = float(previous_start_time + note_row['step'])
        end_time = float(start_time + note_row['duration'])

        # Create a new MIDI note and add it to the instrument
        midi_note = pretty_midi.Note(
            velocity=velocity,
            pitch=int(note_row['pitch']),
            start=start_time,
            end=end_time,
        )
        instrument.notes.append(midi_note)

        # Update the start time for the next note
        previous_start_time = start_time

    # Add the instrument to the PrettyMIDI object
    midi_file.instruments.append(instrument)

    # Write the PrettyMIDI object to a file
    midi_file.write(output_file)

    # Return the PrettyMIDI object
    return midi_file

# Define the path for the output MIDI file
example_file_path = 'example.midi'

# Call the function to convert the notes DataFrame to a MIDI file
# 'instrument_name' should be defined, or replace with the desired instrument name
example_pm = notes_to_midi(
    notes_dataframe,  # DataFrame containing note data
    output_file=example_file_path,  # Path to save the MIDI file
    instrument_name=instrument_name  # Name of the instrument to use
)

import numpy as np
import IPython.display as display

# Convert the PrettyMIDI object to an audio waveform
waveform = example_pm.fluidsynth(fs=AUDIO_SAMPLE_RATE)

# Create an Audio object to display the audio in the notebook
audio_display = display.Audio(waveform, rate=AUDIO_SAMPLE_RATE)

# Display the audio player
audio_display

import glob
import pathlib

# Define the directory where the MIDI files are stored
data_directory = pathlib.Path('data/maestro-v2.0.0')

# Get a list of all MIDI files in the directory
filenames = glob.glob(str(data_directory / '**/*.mid*'))

import glob
import pathlib
import pandas as pd
import pretty_midi

# Define the directory where the MIDI files are stored
data_directory = pathlib.Path('data/maestro-v2.0.0')

# Get a list of all MIDI files in the directory
filenames = glob.glob(str(data_directory / '**/*.mid*'))

# Function to convert MIDI file to DataFrame of notes (example function)
def midi_to_notes(midi_file_path: str) -> pd.DataFrame:
    pm = pretty_midi.PrettyMIDI(midi_file_path)
    notes = []
    for instrument in pm.instruments:
        for note in instrument.notes:
            notes.append({
                'pitch': note.pitch,
                'start': note.start,
                'end': note.end,
                'duration': note.end - note.start,
                'step': note.start  # Assuming step is the same as start time
            })
    return pd.DataFrame(notes)

# Define the number of MIDI files to process
num_files_to_process = 5

# Initialize a list to store the notes data from all files
combined_notes_list = []

# Process each file and convert MIDI to notes
for file_path in filenames[:num_files_to_process]:
    # Convert MIDI file to DataFrame of notes
    notes_dataframe = midi_to_notes(file_path)

    # Append the notes DataFrame to the list
    combined_notes_list.append(notes_dataframe)

# Combine all notes DataFrames into a single DataFrame
all_notes_dataframe = pd.concat(combined_notes_list)

# Get the number of notes in the combined DataFrame
num_notes_parsed = len(all_notes_dataframe)

# Print the number of notes parsed
print('Number of notes parsed:', num_notes_parsed)

import numpy as np

# Define the order of keys/columns to include
key_order = ['pitch', 'step', 'duration']

# Ensure that 'all_notes_dataframe' contains the required columns
for key in key_order:
    if key not in all_notes_dataframe.columns:
        raise ValueError(f"Column '{key}' is missing from the DataFrame.")

# Stack the columns in the specified order into a NumPy array
train_notes = np.stack([all_notes_dataframe[key].values for key in key_order], axis=1)

# Print the shape of the resulting NumPy array
print('Shape of train_notes array:', train_notes.shape)

import tensorflow as tf

# Create a TensorFlow dataset from the NumPy array
notes_ds = tf.data.Dataset.from_tensor_slices(train_notes)

# Print the element specification of the dataset
print('Element specification of notes_ds:', notes_ds.element_spec)

import tensorflow as tf

def create_sequences(
    dataset: tf.data.Dataset,
    seq_length: int,
    vocab_size: int = 128,
) -> tf.data.Dataset:
    """
    Creates a TensorFlow Dataset of sequences and labels from the input dataset.

    Args:
        dataset (tf.data.Dataset): Input dataset of note data.
        seq_length (int): Length of each sequence.
        vocab_size (int): The number of distinct notes (default 128 for MIDI pitches).

    Returns:
        tf.data.Dataset: A dataset of (input_sequences, labels) tuples.
    """
    # Increase sequence length to include labels
    seq_length += 1

    # Create sliding windows of data with the specified sequence length
    windows = dataset.window(seq_length, shift=1, stride=1, drop_remainder=True)

    # Flatten each window into sequences of tensors
    flatten = lambda x: x.batch(seq_length, drop_remainder=True)
    sequences = windows.flat_map(flatten)

    # Function to normalize the pitch values
    def scale_pitch(x):
        x = x / [vocab_size, 1.0, 1.0]  # Normalize pitch to range [0, 1]
        return x

    # Function to split sequences into inputs and labels
    def split_labels(sequences):
        # Separate inputs and labels
        inputs = sequences[:-1]
        labels_dense = sequences[-1]
        labels = {key: labels_dense[i] for i, key in enumerate(key_order)}

        # Normalize inputs
        return scale_pitch(inputs), labels

    # Map the split_labels function to each sequence in the dataset
    return sequences.map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)

import tensorflow as tf

# Define the sequence length and vocabulary size
seq_length = 25
vocab_size = 128

# Create the dataset of sequences and labels
seq_ds = create_sequences(notes_ds, seq_length, vocab_size)

# Print the element specification of the dataset
print('Element specification of seq_ds:', seq_ds.element_spec)

# Take one example from the dataset
for seq, target in seq_ds.take(1):
    # Print the shape of the sequence
    print('Sequence shape:', seq.shape)

    # Print the first 10 elements of the sequence
    print('Sequence elements (first 10):', seq[:10])

    # Print the target labels
    print('Target:', target)

import tensorflow as tf

# Assuming 'all_notes_dataframe' contains the notes data
# Calculate the number of notes
n_notes = len(all_notes_dataframe)

# Define the sequence length and batch size
seq_length = 25
batch_size = 64

# Calculate the buffer size for shuffling
buffer_size = n_notes - seq_length  # the number of items in the dataset

# Create the training dataset with shuffling, batching, caching, and prefetching
train_ds = (
    seq_ds
    .shuffle(buffer_size)              # Shuffle the dataset with buffer size
    .batch(batch_size, drop_remainder=True)  # Batch the dataset
    .cache()                          # Cache the dataset for performance
    .prefetch(tf.data.experimental.AUTOTUNE)  # Prefetch for optimal performance
)

# Print the element specification of the training dataset
print('Element specification of train_ds:', train_ds.element_spec)

import tensorflow as tf

def mse_with_positive_pressure(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
    # Calculate Mean Squared Error
    mse = (y_true - y_pred) ** 2

    # Apply positive pressure penalty to the predictions
    positive_pressure = 10 * tf.maximum(-y_pred, 0.0)

    # Compute the final loss as MSE plus the positive pressure penalty
    return tf.reduce_mean(mse + positive_pressure)

import tensorflow as tf

# Define the input shape and learning rate
input_shape = (seq_length, 3)
learning_rate = 0.005

# Define the model architecture
inputs = tf.keras.Input(shape=input_shape)
x = tf.keras.layers.LSTM(128)(inputs)

outputs = {
    'pitch': tf.keras.layers.Dense(128, name='pitch')(x),
    'step': tf.keras.layers.Dense(1, name='step')(x),
    'duration': tf.keras.layers.Dense(1, name='duration')(x),
}

# Create the model
model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Define the loss functions
loss = {
    'pitch': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    'step': mse_with_positive_pressure,
    'duration': mse_with_positive_pressure,
}

# Define the optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# Compile the model
model.compile(loss=loss, optimizer=optimizer)

# Print model summary
model.summary()

# Evaluate the model on the training dataset
losses = model.evaluate(train_ds, return_dict=True)

# Print the loss values
print('Losses:', losses)

import tensorflow as tf

# Define loss functions
loss = {
    'pitch': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    'step': mse_with_positive_pressure,
    'duration': mse_with_positive_pressure,
}

# Define optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)

# Compile the model with loss weights
model.compile(
    loss=loss,
    loss_weights={
        'pitch': 0.05,
        'step': 1.0,
        'duration': 1.0,
    },
    optimizer=optimizer
)

# Print model summary
model.summary()

# Evaluate the model on the training dataset
losses = model.evaluate(train_ds, return_dict=True)

# Print the loss values for each output
print('Losses:', losses)

import tensorflow as tf

# Define callbacks for training
callbacks = [
    # Save model weights at the end of each epoch
    tf.keras.callbacks.ModelCheckpoint(
        filepath='./training_checkpoints/ckpt_{epoch}.weights.h5',
        save_weights_only=True,
        verbose=1  # Print a message when weights are saved
    ),
    # Stop training when no improvement is observed
    tf.keras.callbacks.EarlyStopping(
        monitor='loss',  # Metric to monitor
        patience=5,  # Number of epochs to wait for improvement
        verbose=1,  # Print a message when early stopping is triggered
        restore_best_weights=True  # Restore weights from the best epoch
    ),
]

# Example of how to use these callbacks in model training
history = model.fit(
    train_ds,
    epochs=50,  # Number of epochs to train
    callbacks=callbacks,
    # Additional arguments like validation_data can be added here
)

# Commented out IPython magic to ensure Python compatibility.
# # Measure the time taken to train the model
# %%time
# 
# epochs = 50
# 
# # Train the model with the defined callbacks
# history = model.fit(
#     train_ds,
#     epochs=epochs,
#     callbacks=callbacks,
# )
#

import matplotlib.pyplot as plt

# Plot the loss over the epochs
plt.plot(history.epoch, history.history['loss'], label='Total Loss')

# Add labels and title
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss Over Time')

# Show the legend
plt.legend()

# Display the plot
plt.show()

def predict_next_note(
    notes: np.ndarray,
    model: tf.keras.Model,
    temperature: float = 1.0) -> tuple[int, float, float]:
  """Generates a note as a tuple of (pitch, step, duration), using a trained sequence model."""

  # Ensure temperature is positive
  assert temperature > 0

  # Add a batch dimension to the input notes
  inputs = tf.expand_dims(notes, 0)

  # Get model predictions for pitch, step, and duration
  predictions = model.predict(inputs)
  pitch_logits = predictions['pitch']
  step = predictions['step']
  duration = predictions['duration']

  # Adjust pitch logits with temperature scaling for diversity
  pitch_logits /= temperature

  # Sample a pitch from the categorical distribution
  pitch = tf.random.categorical(pitch_logits, num_samples=1)
  pitch = tf.squeeze(pitch, axis=-1)

  # Remove unnecessary dimensions from step and duration
  duration = tf.squeeze(duration, axis=-1)
  step = tf.squeeze(step, axis=-1)

  # Ensure that step and duration values are non-negative
  step = tf.maximum(0, step)
  duration = tf.maximum(0, duration)

  # Return the predicted pitch, step, and duration
  return int(pitch), float(step), float(duration)

# Assuming you have already defined the MIDI file and midi_to_notes function
sample_file = filenames[1]  # Or any other file path to a MIDI file
raw_notes = midi_to_notes(sample_file)  # Extract notes from the MIDI file

# Define the key order (pitch, step, duration)
key_order = ['pitch', 'step', 'duration']

# Now proceed with the rest of the code
sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)

# Normalize the pitch for the first sequence of notes
input_notes = (
    sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))

# Continue with the rest of the note generation code

# Assuming 'raw_notes' is extracted from the MIDI file
sample_file = filenames[1]  # Pick a sample MIDI file from the list
raw_notes = midi_to_notes(sample_file)  # Extract notes into a DataFrame

# Ensure the key_order is defined
key_order = ['pitch', 'step', 'duration']
vocab_size = 128
seq_length = 25
temperature = 2.0
num_predictions = 120

# Prepare the initial sequence of notes (pitch, step, duration)
sample_notes = np.stack([raw_notes[key] for key in key_order], axis=1)

# Normalize the pitch for the first sequence of notes
input_notes = (
    sample_notes[:seq_length] / np.array([vocab_size, 1, 1]))

generated_notes = []
prev_start = 0
for _ in range(num_predictions):
    pitch, step, duration = predict_next_note(input_notes, model, temperature)
    start = prev_start + step
    end = start + duration
    input_note = (pitch, step, duration)
    generated_notes.append((*input_note, start, end))
    input_notes = np.delete(input_notes, 0, axis=0)
    input_notes = np.append(input_notes, np.expand_dims(input_note, 0), axis=0)
    prev_start = start

# Convert the generated notes into a DataFrame
generated_notes = pd.DataFrame(
    generated_notes, columns=(*key_order, 'start', 'end'))

# Now display the first 10 generated notes
generated_notes.head(10)

plot_piano_roll(generated_notes)

plot_distributions(generated_notes)

